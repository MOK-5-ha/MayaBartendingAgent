{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoEn83WhhhNSlKuf2Um/vS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gen-ai-capstone-project-bartender-agent/MOK-5-ha/blob/main/notebooks/gradio_ui_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing for Kaggle Submission\n",
        "\n",
        "This notebook is primarily for testing Gradio UI in-notebook. Note that this is not the only valid way in which we can test our use of Gradio, but rather that once we acheieve a desired result in an IDE, we must ensure it can be implemented here as well."
      ],
      "metadata": {
        "id": "AqcjjWCxqTwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvF9erbBqPk9",
        "outputId": "29cb56e4-0072-4447-959d-147d5030730a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai>=0.3.0 in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Collecting langgraph>=0.0.10\n",
            "  Downloading langgraph-0.3.29-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: websockets>=12.0 in /usr/local/lib/python3.11/dist-packages (15.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (9.1.2)\n",
            "Collecting gradio>=4.0.0\n",
            "  Downloading gradio-5.25.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (2.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (4.13.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0) (1.26.1)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph>=0.0.10) (0.3.51)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph>=0.0.10)\n",
            "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph>=0.0.10)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph>=0.0.10)\n",
            "  Downloading langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph>=0.0.10)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0) (2025.1.31)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio>=4.0.0)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio>=4.0.0)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio>=4.0.0)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio>=4.0.0)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio>=4.0.0)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (11.1.0)\n",
            "Collecting pydub (from gradio>=4.0.0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio>=4.0.0)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio>=4.0.0)\n",
            "  Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio>=4.0.0)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>=4.0.0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio>=4.0.0)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio>=4.0.0)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (0.15.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio>=4.0.0)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio>=4.0.0) (2025.3.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0) (1.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai>=0.3.0) (1.69.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0) (4.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio>=4.0.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio>=4.0.0) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio>=4.0.0) (3.18.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph>=0.0.10) (0.3.24)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph>=0.0.10) (1.33)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph>=0.0.10)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio>=4.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio>=4.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio>=4.0.0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.3.0) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.3.0) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0) (13.9.4)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.3.0) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.3.0) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.3.0) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai>=0.3.0) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph>=0.0.10) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph>=0.0.10) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph>=0.0.10) (0.23.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.3.0) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio>=4.0.0) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0) (0.1.2)\n",
            "Downloading langgraph-0.3.29-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.25.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, xxhash, uvicorn, tomlkit, semantic-version, ruff, python-multipart, ormsgpack, groovy, ffmpy, aiofiles, starlette, safehttpx, langgraph-sdk, gradio-client, fastapi, gradio, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.25.0 gradio-client-1.8.0 groovy-0.1.2 langgraph-0.3.29 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 ormsgpack-1.9.1 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.5 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Remove conflicting packages from the Kaggle base environment.\n",
        "!pip uninstall -qqy thinc spacy fastai google-cloud-bigquery\n",
        "!pip install \"google-generativeai>=0.3.0\" \"langgraph>=0.0.10\" \"requests>=2.31.0\" \"websockets>=12.0\" \"tenacity>=8.2.3\" \"gradio>=4.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simply add your Gemini API key to the 'key' icon on the left sidebar and you can run the notebook."
      ],
      "metadata": {
        "id": "adxn5iznrPFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "# UI / Display\n",
        "import gradio as gr\n",
        "# from IPython.display import Markdown, display # Not needed for Gradio script\n",
        "\n",
        "# Gemini - Frontier LLM\n",
        "try:\n",
        "    # Using 'ggenai' alias consistent with user's snippets\n",
        "    import google.generativeai as ggenai\n",
        "    from google.api_core import retry as core_retry # For potential core retries\n",
        "    from google.generativeai import types as genai_types # For specific types if needed later\n",
        "    #from google.generativeai import errors as genai_errors # For specific error handling\n",
        "except ImportError:\n",
        "    print(\"Error: google.generativeai library not found.\")\n",
        "    print(\"Please install it using: pip install google-generativeai\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Tenacity for retries on specific functions\n",
        "from tenacity import (\n",
        "    retry as tenacity_retry, # Alias to avoid confusion with google.api_core.retry\n",
        "    stop_after_attempt,\n",
        "    wait_exponential,\n",
        "    retry_if_exception_type,\n",
        "    before_sleep_log\n",
        ")\n",
        "\n",
        "# Attempt to import userdata for Colab, fallback to environment variables\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    IS_COLAB = True\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "    # Consider adding python-dotenv for local .env file support\n",
        "    # from dotenv import load_dotenv\n",
        "    # load_dotenv()\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Get API Key\n",
        "GOOGLE_API_KEY = None\n",
        "if IS_COLAB:\n",
        "    try:\n",
        "        GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "        logger.info(\"Retrieved GOOGLE_API_KEY from Colab userdata.\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not get GOOGLE_API_KEY from Colab userdata: {e}\")\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    if GOOGLE_API_KEY:\n",
        "        logger.info(\"Retrieved GOOGLE_API_KEY from environment variable.\")\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    logger.error(\"FATAL: GOOGLE_API_KEY not found in Colab userdata or environment variables.\")\n",
        "    # You might want to exit or raise an error here depending on desired behavior\n",
        "    raise EnvironmentError(\"GOOGLE_API_KEY is required but not found.\")\n",
        "\n",
        "\n",
        "# Configure Gemini Client and Model\n",
        "try:\n",
        "    ggenai.configure(api_key=GOOGLE_API_KEY)\n",
        "    # Use a valid and available model name, e.g., 'gemini-1.5-flash' or 'gemini-pro'\n",
        "    # 'gemini-2.0-flash' is not a standard public model name as of late 2024\n",
        "    MODEL_NAME = 'gemini-2.0-flash'\n",
        "    model = ggenai.GenerativeModel(MODEL_NAME)\n",
        "    logger.info(f\"Successfully initialized Gemini model: {MODEL_NAME}\")\n",
        "\n",
        "    # Optional: Apply retry logic directly to the client's method if desired\n",
        "    # is_retriable = lambda e: isinstance(e, genai_errors.ResourceExhaustedError) or \\\n",
        "    #                           isinstance(e, genai_errors.InternalServerError) or \\\n",
        "    #                           isinstance(e, genai_errors.ServiceUnavailableError)\n",
        "    # model.generate_content = core_retry.Retry(predicate=is_retriable)(model.generate_content)\n",
        "    # logger.info(\"Applied google.api_core retry logic to generate_content.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.exception(f\"Fatal: Failed to initialize Gemini model: {str(e)}\")\n",
        "    raise RuntimeError(\n",
        "        f\"Failed to initialize Gemini model. Check API key and model name ('{MODEL_NAME}').\"\n",
        "    ) from e\n",
        "\n",
        "\n",
        "# --- Module-Level State Variables ---\n",
        "\n",
        "# Define the Menu\n",
        "menu: Dict[str, Dict[str, float]] = {\n",
        "    \"1\": {\"name\": \"Old Fashioned\", \"price\": 12.00},\n",
        "    \"2\": {\"name\": \"Margarita\", \"price\": 10.00},\n",
        "    \"3\": {\"name\": \"Mojito\", \"price\": 11.00},\n",
        "    \"4\": {\"name\": \"Martini\", \"price\": 13.00},\n",
        "    \"5\": {\"name\": \"Whiskey Sour\", \"price\": 11.00},\n",
        "    \"6\": {\"name\": \"Gin and Tonic\", \"price\": 9.00},\n",
        "    \"7\": {\"name\": \"Manhattan\", \"price\": 12.00},\n",
        "    \"8\": {\"name\": \"Daiquiri\", \"price\": 10.00},\n",
        "    \"9\": {\"name\": \"Negroni\", \"price\": 11.00},\n",
        "    \"10\": {\"name\": \"Cosmopolitan\", \"price\": 12.00}\n",
        "}\n",
        "\n",
        "# Define Current Order and Conversation History (mutable state)\n",
        "current_order: List[Dict[str, float]] = []\n",
        "# Storing history in the format Gradio expects: [{'role': 'user'/'assistant', 'content': '...'}]\n",
        "conversation_history: List[Dict[str, str]] = []\n",
        "\n",
        "# --- Core Agent Functions ---\n",
        "\n",
        "def get_menu_text() -> str:\n",
        "    \"\"\"Generates the menu text.\"\"\"\n",
        "    global menu\n",
        "    menu_text = \"Menu:\\n\" + \"-\"*5 + \"\\n\"\n",
        "    for item_id, item in menu.items():\n",
        "        menu_text += f\"{item_id}. {item['name']} - ${item['price']:.2f}\\n\"\n",
        "    return menu_text\n",
        "\n",
        "# Define specific exceptions for tenacity retry relevant to API calls\n",
        "# RETRYABLE_EXCEPTIONS = (\n",
        "    # genai_errors.ResourceExhaustedError,\n",
        "    # genai_errors.InternalServerError,\n",
        "    # genai_errors.ServiceUnavailableError,\n",
        "    # Add other potentially transient network errors if needed, e.g., ConnectionError\n",
        "    # ConnectionError, # Be cautious with retrying generic connection errors\n",
        "# )\n",
        "\n",
        "@tenacity_retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=1, min=2, max=10), # Adjusted wait time\n",
        "    # etry=retry_if_exception_type(RETRYABLE_EXCEPTIONS),\n",
        "    before_sleep=before_sleep_log(logger, logging.WARNING),\n",
        "    reraise=True # Re-raise the exception if all retries fail\n",
        ")\n",
        "def _call_gemini_api(prompt_content: List[str], config: Dict) -> ggenai.types.GenerateContentResponse:\n",
        "    \"\"\"Internal function to call the Gemini API with retry logic.\"\"\"\n",
        "    logger.debug(\"Calling Gemini API...\")\n",
        "    response = model.generate_content(\n",
        "        contents=prompt_content, # Correct parameter name is 'contents'\n",
        "        generation_config=config,\n",
        "        # safety_settings can be added here if needed\n",
        "    )\n",
        "    logger.debug(\"Gemini API call successful.\")\n",
        "    return response\n",
        "\n",
        "\n",
        "def process_order(user_input_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Processes user input, calls Gemini, updates state, and returns the response text.\n",
        "    Manages the module-level 'conversation_history' and 'current_order'.\n",
        "    \"\"\"\n",
        "    global conversation_history, current_order, menu, model # Declare modification/access\n",
        "\n",
        "    if not user_input_text:\n",
        "        logger.warning(\"Received empty user input.\")\n",
        "        return \"Please tell me what you'd like to order.\"\n",
        "\n",
        "    try:\n",
        "        # --- Construct the prompt for Gemini ---\n",
        "        # Use the existing conversation history managed globally\n",
        "        prompt_context = [\n",
        "            \"You are a friendly and helpful bartender taking drink orders.\",\n",
        "            \"Be conversational. Ask clarifying questions if the order is unclear.\",\n",
        "            \"If the user asks for something not on the menu, politely tell them and show the menu again.\",\n",
        "            \"If the user asks to see their current order, list the items and their prices.\",\n",
        "            \"\\nHere is the menu:\",\n",
        "            get_menu_text(),\n",
        "            \"\\nCurrent order:\",\n",
        "        ]\n",
        "        if current_order:\n",
        "            order_text = \"\\n\".join([f\"- {item['name']} (${item['price']:.2f})\" for item in current_order])\n",
        "            prompt_context.append(order_text)\n",
        "        else:\n",
        "            prompt_context.append(\"No items ordered yet.\")\n",
        "\n",
        "        prompt_context.append(\"\\nConversation History (latest turns):\")\n",
        "\n",
        "        # Create prompt history from the global history (Gradio format)\n",
        "        history_limit = 10 # Keep the last ~5 pairs of interactions\n",
        "        limited_history_for_prompt = conversation_history[-history_limit:]\n",
        "\n",
        "        for entry in limited_history_for_prompt:\n",
        "             role = entry.get(\"role\", \"unknown\").capitalize()\n",
        "             content = entry.get(\"content\", \"\")\n",
        "             prompt_context.append(f\"{role}: {content}\")\n",
        "\n",
        "        # Add the current user input to the prompt context\n",
        "        prompt_context.append(f\"\\nUser: {user_input_text}\")\n",
        "        prompt_context.append(\"\\nBartender:\") # Ask the model to reply as the bartender\n",
        "\n",
        "        full_prompt = \"\\n\".join(prompt_context)\n",
        "        logger.info(f\"Processing user input: {user_input_text}\")\n",
        "        logger.debug(f\"Full prompt for Gemini:\\n------\\n{full_prompt}\\n------\")\n",
        "\n",
        "        # --- Call the Gemini model via the retry wrapper ---\n",
        "        config_dict = {\n",
        "            'temperature': 0.7,\n",
        "            'max_output_tokens': 2048,\n",
        "            # 'candidate_count': 1 # Usually defaults to 1\n",
        "        }\n",
        "\n",
        "        # Note: 'contents' expects an iterable of 'Content' parts.\n",
        "        # For simple text prompts, passing the string directly often works,\n",
        "        # but wrapping in a list is safer.\n",
        "        response = _call_gemini_api(prompt_content=[full_prompt], config=config_dict)\n",
        "\n",
        "        # --- Process the response ---\n",
        "        agent_response_text = \"\" # Default empty response\n",
        "\n",
        "        # Check response validity and safety\n",
        "        if not response.candidates:\n",
        "             logger.error(\"Gemini response has no candidates.\")\n",
        "             if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                 logger.error(f\"Prompt Blocked: {response.prompt_feedback.block_reason_message}\")\n",
        "                 agent_response_text = f\"I'm sorry, my ability to respond was blocked. Reason: {response.prompt_feedback.block_reason_message or response.prompt_feedback.block_reason}\"\n",
        "             else:\n",
        "                 agent_response_text = \"Sorry, I couldn't generate a response. Please try again.\"\n",
        "\n",
        "        elif not response.candidates[0].content or not response.candidates[0].content.parts:\n",
        "             logger.error(\"Gemini response candidate is empty or has no parts.\")\n",
        "             finish_reason = response.candidates[0].finish_reason\n",
        "             finish_reason_name = finish_reason.name if finish_reason else 'UNKNOWN'\n",
        "             logger.error(f\"Finish Reason: {finish_reason_name}\")\n",
        "\n",
        "             if finish_reason_name == \"SAFETY\":\n",
        "                 agent_response_text = \"I'm sorry, I can't provide that response due to safety reasons.\"\n",
        "             elif finish_reason_name == \"RECITATION\":\n",
        "                 agent_response_text = \"My response couldn't be completed due to potential recitation issues.\"\n",
        "             elif finish_reason_name == \"MAX_TOKENS\":\n",
        "                 # Attempt to get partial text if stopped due to length\n",
        "                 try:\n",
        "                     agent_response_text = response.candidates[0].content.parts[0].text + \"... (response truncated)\"\n",
        "                     logger.warning(\"Response truncated due to max_tokens.\")\n",
        "                 except (AttributeError, IndexError):\n",
        "                     agent_response_text = \"My response was cut short as it reached the maximum length.\"\n",
        "             else:\n",
        "                agent_response_text = f\"Sorry, I had trouble generating a complete response (Finish Reason: {finish_reason_name}). Could you rephrase?\"\n",
        "        else:\n",
        "             # Successfully got response text\n",
        "             agent_response_text = response.candidates[0].content.parts[0].text\n",
        "             logger.info(f\"Gemini response received: {agent_response_text}\")\n",
        "\n",
        "             # --- Update Order Based on Response (Simple Heuristic) ---\n",
        "             # This logic remains basic and might need refinement.\n",
        "             for item_id, item in menu.items():\n",
        "                 item_name_lower = item[\"name\"].lower()\n",
        "                 response_lower = agent_response_text.lower()\n",
        "                 # Check if item name is mentioned and there's indication of adding\n",
        "                 if item_name_lower in response_lower and \\\n",
        "                    any(add_word in response_lower for add_word in [\"added\", \"adding\", \"got it\", \"sure thing\", \"order up\", \"coming right up\"]):\n",
        "                      # Avoid adding duplicates if it's already the *last* item added\n",
        "                      if not current_order or item[\"name\"] != current_order[-1][\"name\"]:\n",
        "                          current_order.append(item)\n",
        "                          logger.info(f\"Heuristic: Added '{item['name']}' to order based on Gemini response.\")\n",
        "                          break # Only add the first match found\n",
        "\n",
        "        # --- Update Global Conversation History ---\n",
        "        # Add the user input *before* the assistant response for correct ordering\n",
        "        # Check if the last entry was already this user's input (e.g., retry scenario)\n",
        "        if not conversation_history or \\\n",
        "           conversation_history[-1]['role'] != 'user' or \\\n",
        "           conversation_history[-1]['content'] != user_input_text:\n",
        "             conversation_history.append({'role': 'user', 'content': user_input_text})\n",
        "\n",
        "        # Add the agent's response\n",
        "        conversation_history.append({'role': 'assistant', 'content': agent_response_text})\n",
        "\n",
        "        return agent_response_text\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch exceptions not handled by tenacity retry (e.g., programming errors, unexpected API issues)\n",
        "        logger.exception(f\"Critical error in process_order: {str(e)}\")\n",
        "        # Provide a safe fallback response\n",
        "        return \"I'm sorry, an unexpected error occurred. Please try again later.\"\n",
        "\n",
        "\n",
        "def reset_order():\n",
        "    \"\"\"Resets the current order and conversation history.\"\"\"\n",
        "    global current_order, conversation_history\n",
        "    try:\n",
        "        current_order = []\n",
        "        conversation_history = [] # Clear the global history\n",
        "        logger.info(\"Order and conversation history reset successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error resetting order state: {str(e)}\")\n",
        "        # This shouldn't really fail, but log just in case\n",
        "        # Avoid raising runtime error here to keep UI responsive if possible\n",
        "\n",
        "\n",
        "# --- Gradio Interface Callbacks ---\n",
        "\n",
        "def handle_gradio_input(user_input, chat_display_history):\n",
        "    \"\"\"\n",
        "    Gradio callback: Takes user input and current UI history,\n",
        "    calls the agent, and returns updated UI state.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Gradio input: '{user_input}'\")\n",
        "    logger.debug(f\"Gradio display history received (type: {type(chat_display_history)}): {chat_display_history}\")\n",
        "\n",
        "    # Call the main processing function which uses/updates global state\n",
        "    # Ensure process_order always returns a string, even on error\n",
        "    response_text = process_order(user_input)\n",
        "    if response_text is None: # Add safety check\n",
        "        logger.error(\"process_order returned None! Defaulting to error message.\")\n",
        "        response_text = \"An internal error occurred (process_order returned None).\"\n",
        "        # Optionally update history here too, or rely on process_order's error handling\n",
        "        if not conversation_history or conversation_history[-1]['role'] != 'assistant':\n",
        "             conversation_history.append({'role': 'assistant', 'content': response_text})\n",
        "\n",
        "\n",
        "    # The global 'conversation_history' is now the source of truth.\n",
        "    # Return the entire updated global history to Gradio.\n",
        "    # Gradio's chatbot component expects a list of dicts: [{'role': 'user'/'assistant', 'content': '...'}]\n",
        "\n",
        "    # --- Added Debugging ---\n",
        "    logger.debug(f\"Value of global conversation_history before return (type: {type(conversation_history)}): {conversation_history}\")\n",
        "    if conversation_history is None:\n",
        "        logger.error(\"CRITICAL: Global conversation_history is None before returning to Gradio!\")\n",
        "        # Decide how to handle this - maybe return empty list to prevent crash?\n",
        "        history_to_return = []\n",
        "    else:\n",
        "        history_to_return = conversation_history\n",
        "    # --- End Added Debugging ---\n",
        "\n",
        "\n",
        "    # Return empty string to clear input box, and the updated history list\n",
        "    return \"\", history_to_return # Use the potentially corrected history_to_return\n",
        "\n",
        "\n",
        "def clear_chat_state():\n",
        "    \"\"\"Gradio callback: Clears backend state and returns empty list for UI.\"\"\"\n",
        "    logger.info(\"Clear button clicked.\")\n",
        "    reset_order() # Clears global current_order and conversation_history\n",
        "    return [] # Return empty list to clear the Gradio chatbot display\n",
        "\n",
        "\n",
        "# --- Gradio UI Definition ---\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo: # Added a theme\n",
        "    gr.Markdown(\"# Bartending Agent\")\n",
        "    gr.Markdown(\"Welcome! Ask me for a drink from the menu or check your order.\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot(\n",
        "                [], # Initialize empty, will be populated by handle_gradio_input's return value\n",
        "                elem_id=\"chatbot\",\n",
        "                label=\"Conversation\",\n",
        "                bubble_full_width=False,\n",
        "                height=500,\n",
        "                # value=conversation_history # Initial value if needed, but usually starts empty\n",
        "                type=\"messages\" # Explicitly set type\n",
        "            )\n",
        "            msg_input = gr.Textbox(\n",
        "                label=\"Your Order / Message\",\n",
        "                placeholder=\"What can I get for you? (e.g., 'I'd like an Old Fashioned', 'What's on the menu?', 'Show my order')\"\n",
        "            )\n",
        "            with gr.Row():\n",
        "                clear_btn = gr.Button(\"Clear Conversation\")\n",
        "                submit_btn = gr.Button(\"Send\", variant=\"primary\") # Added explicit submit button\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "             gr.Markdown(\"### Menu\")\n",
        "             # Display menu dynamically using the function\n",
        "             gr.Markdown(get_menu_text(), elem_id=\"menu-display\")\n",
        "             # Could add components to display current order here if desired\n",
        "\n",
        "    # --- Event Handlers ---\n",
        "    # Link Textbox submit (Enter key) to handle_gradio_input\n",
        "    msg_input.submit(\n",
        "        handle_gradio_input,\n",
        "        [msg_input, chatbot], # Inputs: current message text, current chatbot state\n",
        "        [msg_input, chatbot]  # Outputs: clear message box, updated chatbot state\n",
        "    )\n",
        "\n",
        "    # Link Submit button click to handle_gradio_input\n",
        "    submit_btn.click(\n",
        "        handle_gradio_input,\n",
        "        [msg_input, chatbot],\n",
        "        [msg_input, chatbot]\n",
        "    )\n",
        "\n",
        "    # Link Clear button click to clear_chat_state\n",
        "    clear_btn.click(\n",
        "        clear_chat_state,\n",
        "        None,               # No inputs needed for clear\n",
        "        [chatbot]           # Output: clear the chatbot display\n",
        "    )\n",
        "\n",
        "\n",
        "# --- Launch the Gradio Interface ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Launching Gradio interface...\")\n",
        "    # debug=True enables auto-reloading and more logs (good for dev)\n",
        "    # share=True creates a public link (use False for local-only)\n",
        "    demo.launch(debug=True, share=True)\n",
        "    logger.info(\"Gradio interface closed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "p11H_d4Uqd9A",
        "outputId": "f43e0e6b-98f0-4974-de75-0bf9f37601fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-c66b5ac951e9>:350: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e7779cdbd4f100ab31.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e7779cdbd4f100ab31.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e7779cdbd4f100ab31.gradio.live\n"
          ]
        }
      ]
    }
  ]
}