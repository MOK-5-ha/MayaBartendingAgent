{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOztQR3UVNr9JMXMdFbDDs1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gen-ai-capstone-project-bartender-agent/MOK-5-ha/blob/main/notebooks/gradio_ui_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing for Kaggle Submission\n",
        "\n",
        "This notebook is primarily for testing Gradio UI in-notebook. Note that this is not the only valid way in which we can test our use of Gradio, but rather that once we acheieve a desired result in an IDE, we must ensure it can be implemented here as well."
      ],
      "metadata": {
        "id": "19nLJyZLr6KA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove conflicting packages from the Kaggle base environment.\n",
        "!pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery\n",
        "\n",
        "!pip install -q -U \"google-genai==1.7.0\"\n",
        "\n",
        "# Install langgraph and the packages used in this notebook.\n",
        "!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'\n",
        "\n",
        "# Installing Gradio to deploy our web app\n",
        "!pip install -q gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzUI2rbXur1W",
        "outputId": "5e510702-1089-4876-fb56-906412ffe2cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping libpysal as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping spacy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping ydata-profiling as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping google-cloud-bigquery as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially testing this notebook on Colab before Kaggle. Code completions and other features make this Jupyter environment a preferred one over developing on Kaggle directly. Due to this, we will see warnings such as above."
      ],
      "metadata": {
        "id": "AGSUDsyDvvrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BBPtIMysHwnz"
      },
      "outputs": [],
      "source": [
        "#!pip uninstall -qy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n",
        "#!pip install -U -q \"google-genai==1.7.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "# Data analytics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# UI / Display\n",
        "import gradio as gr\n",
        "from IPython.display import Markdown, display\n",
        "# Gemini - Frontier LLM\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import os\n",
        "\n",
        "genai.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3W-5V_N8r40C",
        "outputId": "b7afa68f-9ed2-4ec0-8fe4-1ce65f9c6948"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.7.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keeping our API Keys Secret\n",
        "\n"
      ],
      "metadata": {
        "id": "cAvejiuTsokC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up your API keyÂ¶\n",
        "To run the following cell, your API key must be stored it in a Kaggle secret named GOOGLE_API_KEY.\n",
        "\n",
        "If you don't already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.\n",
        "\n",
        "To make the key available through Kaggle secrets, choose Secrets from the Add-ons menu and follow the instructions to add your key or enable it for this notebook."
      ],
      "metadata": {
        "id": "7-yEEFDewYLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "822YAcfSwu2t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BartendingAgent:\n",
        "    def __init__(self):\n",
        "        # Check for required environment variables\n",
        "        self.gemini_api_key = os.getenv(\"genai\")\n",
        "        if not self.gemini_api_key:\n",
        "            raise EnvironmentError(\n",
        "                \"GEMINI_API_KEY not found in environment variables. \"\n",
        "                \"Please add it to your .env file.\"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            genai.configure(api_key=self.gemini_api_key)\n",
        "            self.model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "            logger.info(\"Successfully initialized Gemini model\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize Gemini model: {str(e)}\")\n",
        "            raise RuntimeError(\n",
        "                f\"Failed to initialize Gemini model: {str(e)}. \"\n",
        "                \"Please check if your GEMINI_API_KEY is valid.\"\n",
        "            )\n",
        "\n",
        "        self.menu = {\n",
        "            \"1\": {\"name\": \"Old Fashioned\", \"price\": 12.00},\n",
        "            \"2\": {\"name\": \"Margarita\", \"price\": 10.00},\n",
        "            \"3\": {\"name\": \"Mojito\", \"price\": 11.00},\n",
        "            \"4\": {\"name\": \"Martini\", \"price\": 13.00},\n",
        "            \"5\": {\"name\": \"Whiskey Sour\", \"price\": 11.00},\n",
        "            \"6\": {\"name\": \"Gin and Tonic\", \"price\": 9.00},\n",
        "            \"7\": {\"name\": \"Manhattan\", \"price\": 12.00},\n",
        "            \"8\": {\"name\": \"Daiquiri\", \"price\": 10.00},\n",
        "            \"9\": {\"name\": \"Negroni\", \"price\": 11.00},\n",
        "            \"10\": {\"name\": \"Cosmopolitan\", \"price\": 12.00}\n",
        "        }\n",
        "\n",
        "        self.current_order = []\n",
        "        self.conversation_history = []"
      ],
      "metadata": {
        "id": "HzYx2ULK3Vys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bartending_agent.py"
      ],
      "metadata": {
        "id": "ssyvauFj0xFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Optional\n",
        "from langgraph.graph import Graph, StateGraph\n",
        "from langgraph.prebuilt import ToolNode\n",
        "import logging\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_exponential,\n",
        "    retry_if_exception_type,\n",
        "    before_sleep_log\n",
        ")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class BartendingAgent:\n",
        "    def __init__(self):\n",
        "        # Check for required environment variables\n",
        "        self.model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "        logger.info(\"Successfully initialized Gemini model\")\n",
        "\n",
        "        self.menu = {\n",
        "            \"1\": {\"name\": \"Old Fashioned\", \"price\": 12.00},\n",
        "            \"2\": {\"name\": \"Margarita\", \"price\": 10.00},\n",
        "            \"3\": {\"name\": \"Mojito\", \"price\": 11.00},\n",
        "            \"4\": {\"name\": \"Martini\", \"price\": 13.00},\n",
        "            \"5\": {\"name\": \"Whiskey Sour\", \"price\": 11.00},\n",
        "            \"6\": {\"name\": \"Gin and Tonic\", \"price\": 9.00},\n",
        "            \"7\": {\"name\": \"Manhattan\", \"price\": 12.00},\n",
        "            \"8\": {\"name\": \"Daiquiri\", \"price\": 10.00},\n",
        "            \"9\": {\"name\": \"Negroni\", \"price\": 11.00},\n",
        "            \"10\": {\"name\": \"Cosmopolitan\", \"price\": 12.00}\n",
        "        }\n",
        "\n",
        "        self.current_order = []\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def get_menu_text(self) -> str:\n",
        "        menu_text = \"Here's our menu:\\n\"\n",
        "        for item_id, item in self.menu.items():\n",
        "            menu_text += f\"{item_id}. {item['name']} - ${item['price']:.2f}\\n\"\n",
        "        return menu_text\n",
        "\n",
        "    @retry(\n",
        "        stop=stop_after_attempt(3),\n",
        "        wait=wait_exponential(multiplier=1, min=4, max=10),\n",
        "        retry=retry_if_exception_type((ConnectionError, TimeoutError)),\n",
        "        before_sleep=before_sleep_log(logger, logging.WARNING),\n",
        "        reraise=True\n",
        "    )\n",
        "    def process_order(self, text: str) -> str:\n",
        "        \"\"\"Processes user input using the Gemini model for conversational interaction.\"\"\"\n",
        "        try:\n",
        "            # Add the user's input to conversation history\n",
        "            self.conversation_history.append({\"role\": \"user\", \"parts\": [text]})\n",
        "            logger.info(f\"Processing user input: {text}\")\n",
        "\n",
        "            # --- Construct the prompt for Gemini ---\n",
        "            prompt_parts = [\n",
        "                \"You are a friendly and helpful bartender taking drink orders.\",\n",
        "                \"Be conversational. Ask clarifying questions if the order is unclear.\",\n",
        "                \"If the user asks for something not on the menu, politely tell them and show the menu again.\",\n",
        "                \"If the user asks to see their current order, list the items and their prices.\",\n",
        "                \"\\nHere is the menu:\",\n",
        "                self.get_menu_text(),\n",
        "                \"\\nCurrent order:\",\n",
        "            ]\n",
        "            if self.current_order:\n",
        "                order_text = \"\\n\".join([f\"- {item['name']} (${item['price']:.2f})\" for item in self.current_order])\n",
        "                prompt_parts.append(order_text)\n",
        "            else:\n",
        "                prompt_parts.append(\"No items ordered yet.\")\n",
        "\n",
        "            prompt_parts.append(\"\\nConversation History:\")\n",
        "            # Add previous turns, limiting history length if necessary\n",
        "            history_limit = 10 # Keep the last 10 turns\n",
        "            limited_history = self.conversation_history[-(history_limit + 1):-1] # Exclude the latest user input already added\n",
        "\n",
        "            for entry in limited_history:\n",
        "                 # Ensure history parts are strings. Handle potential list/dict structures if they exist.\n",
        "                 content = entry.get(\"parts\", [\"\"])[0] # Assuming parts is a list with one string\n",
        "                 if isinstance(content, dict): # Handle cases where parts might be structured differently\n",
        "                     content = str(content) # Fallback to string representation\n",
        "                 role = entry.get(\"role\", \"unknown\")\n",
        "                 prompt_parts.append(f\"{role.capitalize()}: {content}\")\n",
        "\n",
        "\n",
        "            prompt_parts.append(f\"\\nUser: {text}\")\n",
        "            prompt_parts.append(\"\\nBartender:\")\n",
        "\n",
        "            full_prompt = \"\\n\".join(prompt_parts)\n",
        "            logger.debug(f\"Full prompt for Gemini:\\n{full_prompt}\")\n",
        "\n",
        "            # --- Call the Gemini model ---\n",
        "            # Use stream=False for a single response object\n",
        "            # Safety settings can be configured here if needed\n",
        "            generation_config = genai.types.GenerationConfig(\n",
        "                # candidate_count=1, # Default is 1\n",
        "                # stop_sequences=None,\n",
        "                # max_output_tokens=2048, # Already set in .env? Agent could load this.\n",
        "                temperature=0.7 # Already set in .env? Agent could load this.\n",
        "            )\n",
        "\n",
        "            response = self.model.generate_content(\n",
        "                contents=[full_prompt], # Send as a list for single-turn\n",
        "                generation_config=generation_config,\n",
        "                # safety_settings='HARM_BLOCK_THRESHOLD_UNSPECIFIED' # Adjust safety if needed\n",
        "            )\n",
        "\n",
        "            # --- Process the response ---\n",
        "            if not response.candidates or not response.candidates[0].content.parts:\n",
        "                 logger.error(\"Gemini response was empty or invalid.\")\n",
        "                 agent_response_text = \"Sorry, I had trouble understanding that. Could you please rephrase?\"\n",
        "            else:\n",
        "                 agent_response_text = response.candidates[0].content.parts[0].text\n",
        "                 logger.info(f\"Gemini response: {agent_response_text}\")\n",
        "\n",
        "                 # Simple check if the *model's response* mentions adding an item\n",
        "                 # This is basic; a more robust approach might involve asking the model\n",
        "                 # to output structured data or using function calling.\n",
        "                 for item_id, item in self.menu.items():\n",
        "                     if item[\"name\"].lower() in agent_response_text.lower() and \\\n",
        "                        any(add_word in agent_response_text.lower() for add_word in [\"added\", \"adding\", \"got it\", \"sure thing\"]):\n",
        "                          # Avoid adding duplicates if already in order from this turn?\n",
        "                          # This logic might need refinement based on model behavior.\n",
        "                          if not self.current_order or item[\"name\"] != self.current_order[-1][\"name\"]:\n",
        "                              self.current_order.append(item)\n",
        "                              logger.info(f\"Added '{item['name']}' to order based on Gemini response.\")\n",
        "                              # Only add the first match found in the response?\n",
        "                              break\n",
        "\n",
        "            # Add the agent's response to conversation history\n",
        "            self.conversation_history.append({\"role\": \"assistant\", \"parts\": [agent_response_text]})\n",
        "\n",
        "            return agent_response_text\n",
        "\n",
        "        # except genai.types.BlockedPromptError as e: # Requires specific import if needed\n",
        "        #     logger.error(f\"Gemini prompt blocked: {e}\")\n",
        "        #     return \"I'm sorry, I can't respond to that request due to safety guidelines.\"\n",
        "        # except genai.types.StopCandidateException as e: # Requires specific import if needed\n",
        "        #     logger.error(f\"Gemini response stopped: {e}\")\n",
        "        #     # The partial response might be in e.response\n",
        "        #     return response.candidates[0].content.parts[0].text if response and response.candidates else \"My response was cut short.\"\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"Error processing order with Gemini: {str(e)}\") # Use logger.exception for traceback\n",
        "            # Fallback response\n",
        "            return \"I'm sorry, there was an internal error processing your request. Please try again.\"\n",
        "\n",
        "    @retry(\n",
        "        stop=stop_after_attempt(3),\n",
        "        wait=wait_exponential(multiplier=1, min=4, max=10),\n",
        "        retry=retry_if_exception_type((ConnectionError, TimeoutError)),\n",
        "        before_sleep=before_sleep_log(logger, logging.WARNING),\n",
        "        reraise=True\n",
        "    )\n",
        "    #def get_voice_response(self, text: str) -> str:\n",
        "        #try:\n",
        "            #if not self.cartesia_api_key:\n",
        "                #raise RuntimeError(\n",
        "                    #\"Cannot generate voice response: CARTESIA_API_KEY is not set\"\n",
        "                #)\n",
        "\n",
        "            #logger.info(f\"Generating voice response for text: {text}\")\n",
        "\n",
        "            # This would be where you'd call the Cartesia API\n",
        "            # For now, we'll just return the text\n",
        "            #return text\n",
        "        #except Exception as e:\n",
        "            #logger.error(f\"Error generating voice response: {str(e)}\")\n",
        "            #raise RuntimeError(\n",
        "                #f\"Failed to generate voice response: {str(e)}. \"\n",
        "                #\"Please check your CARTESIA_API_KEY and network connection.\"\n",
        "            #)\n",
        "\n",
        "    def reset_order(self):\n",
        "        try:\n",
        "            self.current_order = []\n",
        "            self.conversation_history = []\n",
        "            logger.info(\"Order and conversation history reset successfully\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error resetting order: {str(e)}\")\n",
        "            raise RuntimeError(f\"Failed to reset order: {str(e)}\")"
      ],
      "metadata": {
        "id": "LQsuQyhlsev3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main.py"
      ],
      "metadata": {
        "id": "uK884KdC07T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Configure logging (optional but recommended)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize the Bartending Agent\n",
        "try:\n",
        "    agent = BartendingAgent()\n",
        "    logger.info(\"Bartending Agent initialized successfully.\")\n",
        "except EnvironmentError as e:\n",
        "    logger.error(f\"EnvironmentError initializing agent: {e}\")\n",
        "    # Exit or provide a dummy agent if API keys are missing\n",
        "    # For now, we'll re-raise to make the issue clear\n",
        "    raise\n",
        "except RuntimeError as e:\n",
        "    logger.error(f\"RuntimeError initializing agent: {e}\")\n",
        "    # Handle other initialization errors (e.g., Gemini connection)\n",
        "    raise\n",
        "\n",
        "# Define the function that Gradio will call\n",
        "def handle_order(user_input, chat_history):\n",
        "    logger.info(f\"Received user input: {user_input}\")\n",
        "\n",
        "    # Process the order using the agent\n",
        "    response_text = agent.process_order(user_input)\n",
        "    logger.info(f\"Agent response text: {response_text}\")\n",
        "\n",
        "    # Optionally, generate voice response (if Cartesia API is configured and needed)\n",
        "    # voice_response = agent.get_voice_response(response_text)\n",
        "    # logger.info(f\"Agent voice response generated (placeholder): {voice_response}\")\n",
        "\n",
        "    # Update chat history\n",
        "    chat_history.append((user_input, response_text))\n",
        "\n",
        "    # Return updated chat history and potentially the voice response\n",
        "    # For now, just returning text response updates\n",
        "    return \"\", chat_history # Return empty string to clear input box\n",
        "\n",
        "# Define the Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Bartending Agent\")\n",
        "    gr.Markdown(agent.get_menu_text()) # Display the menu\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Conversation\", value=[]) # Initialize chatbot display\n",
        "\n",
        "    msg = gr.Textbox(label=\"Your Order\", placeholder=\"What can I get for you?\")\n",
        "\n",
        "    clear = gr.Button(\"Clear Conversation\")\n",
        "\n",
        "    msg.submit(handle_order, [msg, chatbot], [msg, chatbot])\n",
        "    clear.click(lambda: (agent.reset_order(), []), None, [chatbot], queue=False)\n",
        "\n",
        "# Launch the Gradio interface\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Launching Gradio interface...\")\n",
        "    # Set share=True to create a public link (optional)\n",
        "    demo.launch(share=True)\n",
        "    logger.info(\"Gradio interface launched.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "FgoBffI_0_oC",
        "outputId": "8840f7aa-d4c4-4100-c7a4-cd2f637796c6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-88977ad0fd75>:45: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Conversation\", value=[]) # Initialize chatbot display\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e54c4e39d858b0167e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e54c4e39d858b0167e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}