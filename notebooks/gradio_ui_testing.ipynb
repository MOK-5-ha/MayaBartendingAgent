{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT6W/nCrMU7fpmCHwiVbJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gen-ai-capstone-project-bartender-agent/MOK-5-ha/blob/main/notebooks/gradio_ui_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing for Kaggle Submission\n",
        "\n",
        "This notebook is primarily for testing Gradio UI in-notebook. Note that this is not the only valid way in which we can test our use of Gradio, but rather that once we acheieve a desired result in an IDE, we must ensure it can be implemented here as well."
      ],
      "metadata": {
        "id": "AqcjjWCxqTwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvF9erbBqPk9",
        "outputId": "6be2f3e9-b17e-4cb1-9478-570193955a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai>=0.3.0 in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Collecting langgraph>=0.0.10\n",
            "  Downloading langgraph-0.3.29-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: websockets>=12.0 in /usr/local/lib/python3.11/dist-packages (15.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (9.1.2)\n",
            "Collecting gradio>=4.0.0\n",
            "  Downloading gradio-5.25.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (2.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.3.0) (4.13.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0) (1.26.1)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph>=0.0.10) (0.3.51)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph>=0.0.10)\n",
            "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph>=0.0.10)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph>=0.0.10)\n",
            "  Downloading langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph>=0.0.10)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0) (2025.1.31)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio>=4.0.0)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio>=4.0.0)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio>=4.0.0)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio>=4.0.0)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio>=4.0.0)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (11.1.0)\n",
            "Collecting pydub (from gradio>=4.0.0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio>=4.0.0)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio>=4.0.0)\n",
            "  Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio>=4.0.0)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>=4.0.0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio>=4.0.0)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio>=4.0.0)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0) (0.15.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio>=4.0.0)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio>=4.0.0) (2025.3.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0) (1.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai>=0.3.0) (1.69.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0) (4.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio>=4.0.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio>=4.0.0) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio>=4.0.0) (3.18.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph>=0.0.10) (0.3.24)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph>=0.0.10) (1.33)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph>=0.0.10)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio>=4.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio>=4.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio>=4.0.0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.3.0) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.3.0) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0) (13.9.4)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.3.0) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.3.0) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.3.0) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai>=0.3.0) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph>=0.0.10) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph>=0.0.10) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph>=0.0.10) (0.23.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.3.0) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio>=4.0.0) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0) (0.1.2)\n",
            "Downloading langgraph-0.3.29-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.25.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, xxhash, uvicorn, tomlkit, semantic-version, ruff, python-multipart, ormsgpack, groovy, ffmpy, aiofiles, starlette, safehttpx, langgraph-sdk, gradio-client, fastapi, gradio, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.25.0 gradio-client-1.8.0 groovy-0.1.2 langgraph-0.3.29 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 ormsgpack-1.9.1 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.5 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Remove conflicting packages from the Kaggle base environment.\n",
        "!pip uninstall -qqy thinc spacy fastai google-cloud-bigquery\n",
        "!pip install \"google-generativeai>=0.3.0\" \"langgraph>=0.0.10\" \"requests>=2.31.0\" \"websockets>=12.0\" \"tenacity>=8.2.3\" \"gradio>=4.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simply add your Gemini API key to the 'key' icon on the left sidebar and you can run the notebook."
      ],
      "metadata": {
        "id": "adxn5iznrPFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "# UI / Display\n",
        "import gradio as gr\n",
        "\n",
        "# Gemini - Frontier LLM\n",
        "try:\n",
        "    import google.generativeai as ggenai\n",
        "    from google.api_core import retry as core_retry\n",
        "    from google.generativeai import types as genai_types\n",
        "    #from google.generativeai import errors as genai_errors\n",
        "except ImportError:\n",
        "    print(\"Error: google.generativeai library not found.\")\n",
        "    print(\"Please install it using: pip install google-generativeai\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Tenacity for retries\n",
        "from tenacity import (\n",
        "    retry as tenacity_retry,\n",
        "    stop_after_attempt,\n",
        "    wait_exponential,\n",
        "    retry_if_exception_type,\n",
        "    before_sleep_log\n",
        ")\n",
        "\n",
        "# Attempt to import userdata for Colab, fallback to environment variables\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    IS_COLAB = True\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "    # from dotenv import load_dotenv # Uncomment if using python-dotenv locally\n",
        "    # load_dotenv()\n",
        "\n",
        "# --- Configuration ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Get API Key\n",
        "GOOGLE_API_KEY = None\n",
        "if IS_COLAB:\n",
        "    try:\n",
        "        GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "        logger.info(\"Retrieved GOOGLE_API_KEY from Colab userdata.\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not get GOOGLE_API_KEY from Colab userdata: {e}\")\n",
        "if not GOOGLE_API_KEY:\n",
        "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    if GOOGLE_API_KEY:\n",
        "        logger.info(\"Retrieved GOOGLE_API_KEY from environment variable.\")\n",
        "if not GOOGLE_API_KEY:\n",
        "    logger.error(\"FATAL: GOOGLE_API_KEY not found in Colab userdata or environment variables.\")\n",
        "    raise EnvironmentError(\"GOOGLE_API_KEY is required but not found.\")\n",
        "\n",
        "# Configure Gemini Client and Model (Initialized ONCE)\n",
        "try:\n",
        "    ggenai.configure(api_key=GOOGLE_API_KEY)\n",
        "    MODEL_NAME = 'gemini-1.5-flash' # Verify this model name\n",
        "    model = ggenai.GenerativeModel(MODEL_NAME)\n",
        "    logger.info(f\"Successfully initialized Gemini model: {MODEL_NAME}\")\n",
        "except Exception as e:\n",
        "    logger.exception(f\"Fatal: Failed to initialize Gemini model: {str(e)}\")\n",
        "    raise RuntimeError(f\"Failed to initialize Gemini model. Check API key and model name ('{MODEL_NAME}').\") from e\n",
        "\n",
        "# --- Static Data ---\n",
        "# Define the Menu (Doesn't change per session)\n",
        "menu: Dict[str, Dict[str, float]] = {\n",
        "    \"1\": {\"name\": \"Old Fashioned\", \"price\": 12.00},\n",
        "    \"2\": {\"name\": \"Margarita\", \"price\": 10.00},\n",
        "    # ... (rest of the menu items) ...\n",
        "    \"9\": {\"name\": \"Negroni\", \"price\": 11.00},\n",
        "    \"10\": {\"name\": \"Cosmopolitan\", \"price\": 12.00}\n",
        "}\n",
        "\n",
        "# --- Core Agent Logic (Now Stateless Functions) ---\n",
        "\n",
        "def get_menu_text() -> str:\n",
        "    \"\"\"Generates the menu text (Stateless).\"\"\"\n",
        "    # No change needed, doesn't access mutable state\n",
        "    menu_text = \"Menu:\\n\" + \"-\"*5 + \"\\n\"\n",
        "    for item_id, item in menu.items():\n",
        "        menu_text += f\"{item_id}. {item['name']} - ${item['price']:.2f}\\n\"\n",
        "    return menu_text\n",
        "\n",
        "# Define retryable exceptions for the API call\n",
        "# RETRYABLE_EXCEPTIONS = (\n",
        "    # genai_errors.ResourceExhaustedError,\n",
        "    # genai_errors.InternalServerError,\n",
        "    # genai_errors.ServiceUnavailableError,\n",
        "#)\n",
        "\n",
        "@tenacity_retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
        "    #retry=retry_if_exception_type(RETRYABLE_EXCEPTIONS),\n",
        "    before_sleep=before_sleep_log(logger, logging.WARNING),\n",
        "    reraise=True\n",
        ")\n",
        "def _call_gemini_api(prompt_content: List[str], config: Dict) -> ggenai.types.GenerateContentResponse:\n",
        "    \"\"\"Internal function to call the Gemini API with retry logic (Stateless).\"\"\"\n",
        "    logger.debug(\"Calling Gemini API...\")\n",
        "    # Uses the globally initialized 'model'\n",
        "    response = model.generate_content(\n",
        "        contents=prompt_content,\n",
        "        generation_config=config,\n",
        "    )\n",
        "    logger.debug(\"Gemini API call successful.\")\n",
        "    return response\n",
        "\n",
        "def process_order(\n",
        "    user_input_text: str,\n",
        "    current_session_history: List[Dict[str, str]],\n",
        "    current_session_order: List[Dict[str, float]]\n",
        ") -> Tuple[str, List[Dict[str, str]], List[Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Processes user input using Gemini, updates state for the CURRENT SESSION.\n",
        "    Accepts session history and order, returns (response_text, updated_history, updated_order).\n",
        "    \"\"\"\n",
        "    if not user_input_text:\n",
        "        logger.warning(\"Received empty user input.\")\n",
        "        # Return current state unchanged with a message\n",
        "        return \"Please tell me what you'd like to order.\", current_session_history, current_session_order\n",
        "\n",
        "    # Local copies for modification within this function call\n",
        "    updated_history = current_session_history[:]\n",
        "    updated_order = current_session_order[:]\n",
        "\n",
        "    try:\n",
        "        # --- Construct the prompt using session-specific history/order ---\n",
        "        prompt_context = [\n",
        "            \"You are a friendly and helpful bartender...\", # Keep instructions\n",
        "            \"\\nHere is the menu:\",\n",
        "            get_menu_text(),\n",
        "            \"\\nCurrent order:\",\n",
        "        ]\n",
        "        if updated_order: # Use the passed-in order\n",
        "            order_text = \"\\n\".join([f\"- {item['name']} (${item['price']:.2f})\" for item in updated_order])\n",
        "            prompt_context.append(order_text)\n",
        "        else:\n",
        "            prompt_context.append(\"No items ordered yet.\")\n",
        "\n",
        "        prompt_context.append(\"\\nConversation History (latest turns):\")\n",
        "        history_limit = 10\n",
        "        limited_history_for_prompt = updated_history[-history_limit:] # Use passed-in history\n",
        "        for entry in limited_history_for_prompt:\n",
        "             role = entry.get(\"role\", \"unknown\").capitalize()\n",
        "             content = entry.get(\"content\", \"\")\n",
        "             prompt_context.append(f\"{role}: {content}\")\n",
        "\n",
        "        prompt_context.append(f\"\\nUser: {user_input_text}\")\n",
        "        prompt_context.append(\"\\nBartender:\")\n",
        "\n",
        "        full_prompt = \"\\n\".join(prompt_context)\n",
        "        logger.info(f\"Processing user input for session: {user_input_text}\")\n",
        "        logger.debug(f\"Full prompt for Gemini:\\n------\\n{full_prompt}\\n------\")\n",
        "\n",
        "        # --- Call Gemini API ---\n",
        "        config_dict = {'temperature': 0.7, 'max_output_tokens': 2048}\n",
        "        response = _call_gemini_api(prompt_content=[full_prompt], config=config_dict)\n",
        "\n",
        "        # --- Process Response ---\n",
        "        agent_response_text = \"\"\n",
        "        # (Error/Safety checking logic for 'response' remains largely the same as before)\n",
        "        if not response.candidates:\n",
        "             logger.error(\"Gemini response has no candidates.\")\n",
        "             # ... (handle blocked prompts, etc.) ...\n",
        "             agent_response_text = \"Sorry, I couldn't generate a response. Please try again.\"\n",
        "        elif not response.candidates[0].content or not response.candidates[0].content.parts:\n",
        "             logger.error(\"Gemini response candidate is empty.\")\n",
        "             # ... (handle finish reasons like SAFETY, MAX_TOKENS etc.) ...\n",
        "             agent_response_text = \"Sorry, I had trouble generating a complete response.\"\n",
        "        else:\n",
        "             agent_response_text = response.candidates[0].content.parts[0].text\n",
        "             logger.info(f\"Gemini response received: {agent_response_text}\")\n",
        "\n",
        "             # --- Update Order Based on Response (Heuristic) ---\n",
        "             # Modifies the 'updated_order' local variable\n",
        "             for item_id, item in menu.items():\n",
        "                 item_name_lower = item[\"name\"].lower()\n",
        "                 response_lower = agent_response_text.lower()\n",
        "                 if item_name_lower in response_lower and \\\n",
        "                    any(add_word in response_lower for add_word in [\"added\", \"adding\", \"got it\", \"sure thing\"]):\n",
        "                      if not updated_order or item[\"name\"] != updated_order[-1][\"name\"]:\n",
        "                          updated_order.append(item) # Append to local copy\n",
        "                          logger.info(f\"Heuristic: Added '{item['name']}' to session order.\")\n",
        "                          break\n",
        "\n",
        "        # --- Update Session History ---\n",
        "        # Append user input and assistant response to the local history copy\n",
        "        updated_history.append({'role': 'user', 'content': user_input_text})\n",
        "        updated_history.append({'role': 'assistant', 'content': agent_response_text})\n",
        "\n",
        "        # --- Return updated state for this session ---\n",
        "        return agent_response_text, updated_history, updated_order\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Critical error in process_order: {str(e)}\")\n",
        "        # Return current state unchanged and an error message\n",
        "        error_message = \"I'm sorry, an unexpected error occurred. Please try again later.\"\n",
        "        # Append only the error message to history to inform the user\n",
        "        updated_history.append({'role': 'user', 'content': user_input_text}) # Keep user msg\n",
        "        updated_history.append({'role': 'assistant', 'content': error_message})\n",
        "        return error_message, updated_history, updated_order\n",
        "\n",
        "# Note: reset_order function is removed as state reset is handled by Gradio callbacks\n",
        "\n",
        "\n",
        "# --- Gradio Interface Callbacks (Using Session State) ---\n",
        "\n",
        "def handle_gradio_input(\n",
        "    user_input: str,\n",
        "    session_history_state: List[Dict[str, str]],\n",
        "    session_order_state: List[Dict[str, float]]\n",
        ") -> Tuple[str, List[Dict[str, str]], List[Dict[str, str]], List[Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Gradio callback: Takes user input and session state, calls the agent,\n",
        "    and returns updated UI and session state.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Gradio input: '{user_input}'\")\n",
        "    logger.debug(f\"Received session history state (len {len(session_history_state)}): {session_history_state}\")\n",
        "    logger.debug(f\"Received session order state (len {len(session_order_state)}): {session_order_state}\")\n",
        "\n",
        "    # Call the stateless processing function with the current session's state\n",
        "    response_text, updated_history, updated_order = process_order(\n",
        "        user_input,\n",
        "        session_history_state,\n",
        "        session_order_state\n",
        "    )\n",
        "\n",
        "    # Return values to update Gradio components:\n",
        "    # 1. Textbox value (clear it)\n",
        "    # 2. Chatbot value (the updated history)\n",
        "    # 3. History state value (persist updated history for next turn)\n",
        "    # 4. Order state value (persist updated order for next turn)\n",
        "    return \"\", updated_history, updated_history, updated_order\n",
        "\n",
        "def clear_chat_state() -> Tuple[List, List, List]:\n",
        "    \"\"\"Gradio callback: Clears UI and session state by returning initial values.\"\"\"\n",
        "    logger.info(\"Clear button clicked - Resetting session state.\")\n",
        "    # Return empty lists for Chatbot, history state, and order state\n",
        "    return [], [], []\n",
        "\n",
        "\n",
        "# --- Gradio UI Definition (with gr.State) ---\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Bartending Agent\")\n",
        "    gr.Markdown(\"Welcome! Each session is independent. Ask me for a drink or check your order.\")\n",
        "\n",
        "    # --- Define Session State Variables ---\n",
        "    # These hold the history and order specific to each user's session\n",
        "    history_state = gr.State([]) # Initialize state for conversation history\n",
        "    order_state = gr.State([])   # Initialize state for current order\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            # Chatbot display - its value is directly controlled by handle_gradio_input's return\n",
        "            chatbot_display = gr.Chatbot(\n",
        "                [], # Start empty, value comes from handle_gradio_input\n",
        "                elem_id=\"chatbot\",\n",
        "                label=\"Conversation\",\n",
        "                bubble_full_width=False,\n",
        "                height=500,\n",
        "                type=\"messages\"\n",
        "            )\n",
        "            msg_input = gr.Textbox(\n",
        "                label=\"Your Order / Message\",\n",
        "                placeholder=\"What can I get for you?\"\n",
        "            )\n",
        "            with gr.Row():\n",
        "                clear_btn = gr.Button(\"Clear Conversation\")\n",
        "                submit_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "             gr.Markdown(\"### Menu\")\n",
        "             gr.Markdown(get_menu_text(), elem_id=\"menu-display\")\n",
        "             # Future enhancement: Could add a gr.Textbox or gr.JSON here\n",
        "             # bound to order_state to display the current order live.\n",
        "\n",
        "    # --- Event Handlers ---\n",
        "    # Define inputs and outputs including the state variables\n",
        "    submit_inputs = [msg_input, history_state, order_state]\n",
        "    submit_outputs = [msg_input, chatbot_display, history_state, order_state]\n",
        "\n",
        "    # Link Textbox submit (Enter key)\n",
        "    msg_input.submit(handle_gradio_input, submit_inputs, submit_outputs)\n",
        "\n",
        "    # Link Submit button click\n",
        "    submit_btn.click(handle_gradio_input, submit_inputs, submit_outputs)\n",
        "\n",
        "    # Link Clear button click\n",
        "    clear_outputs = [chatbot_display, history_state, order_state]\n",
        "    clear_btn.click(clear_chat_state, None, clear_outputs)\n",
        "\n",
        "\n",
        "# --- Launch the Gradio Interface ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Launching Gradio interface...\")\n",
        "    demo.launch(debug=True, share=True) # Set share=True if public link needed\n",
        "    logger.info(\"Gradio interface closed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "p11H_d4Uqd9A",
        "outputId": "f9311587-eb6c-4603-9d6a-18520eed77ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-abbb39028a80>:265: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot_display = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6355171aa12693a125.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6355171aa12693a125.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://6355171aa12693a125.gradio.live\n"
          ]
        }
      ]
    }
  ]
}