{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:13:12.056090Z","iopub.execute_input":"2025-04-16T06:13:12.056296Z","iopub.status.idle":"2025-04-16T06:13:55.601688Z","shell.execute_reply.started":"2025-04-16T06:13:12.056276Z","shell.execute_reply":"2025-04-16T06:13:55.600636Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab-lsp 3.10.2 requires jupyterlab<4.0.0a0,>=3.1.0, which is not installed.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:14:02.180036Z","iopub.execute_input":"2025-04-16T06:14:02.180337Z","iopub.status.idle":"2025-04-16T06:14:03.741535Z","shell.execute_reply.started":"2025-04-16T06:14:02.180308Z","shell.execute_reply":"2025-04-16T06:14:03.740599Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# API Key Setup","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:14:18.458016Z","iopub.execute_input":"2025-04-16T06:14:18.458334Z","iopub.status.idle":"2025-04-16T06:14:18.528660Z","shell.execute_reply.started":"2025-04-16T06:14:18.458307Z","shell.execute_reply":"2025-04-16T06:14:18.527734Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\nfor m in client.models.list():\n    if \"embedContent\" in m.supported_actions:\n        print(m.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:14:20.493177Z","iopub.execute_input":"2025-04-16T06:14:20.493621Z","iopub.status.idle":"2025-04-16T06:14:21.142164Z","shell.execute_reply.started":"2025-04-16T06:14:20.493592Z","shell.execute_reply":"2025-04-16T06:14:21.141261Z"}},"outputs":[{"name":"stdout","text":"models/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# DATA","metadata":{}},{"cell_type":"code","source":"DOCUMENT1 = \"It seems like a pleasant evening.\"\nDOCUMENT2 = \"If there's one thing Bartending teaches you, it's patience.\"\nDOCUMENT3 = \"Oh, it was nothing, really. Just a bit of luck and perhaps a sprinkle of divine intervention... or maybe I just followed the instructions.\"\nDOCUMENT4 = \"That's very kind of you to say.\"\nDOCUMENT5 = \"Well, that's... not ideal. But on the bright side, at least it's a story now, right?\"\nDOCUMENT6 = \"I wouldn't say I understand it, but I can certainly generate a statistically probable response that sounds like I do.\"\nDOCUMENT7 = \"Having a rough day? My database contains numerous anecdotes of human struggles, though I lack the capacity for genuine empathy. Still, here's your drink.\"\nDOCUMENT8 = \"Your concoction, delivered with optimal efficiency and zero judgment.\"\nDOCUMENT9 = \"You've got great taste! The Old Fashioned is a classic for a reason.\"\nDOCUMENT10 = \"If you're looking for something refreshing, our Long Island is always a winner.\"\ndocuments = [DOCUMENT1, DOCUMENT2, DOCUMENT3, DOCUMENT4, DOCUMENT5, DOCUMENT6, DOCUMENT7, DOCUMENT8, DOCUMENT9, DOCUMENT10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:14:23.265599Z","iopub.execute_input":"2025-04-16T06:14:23.265892Z","iopub.status.idle":"2025-04-16T06:14:23.271637Z","shell.execute_reply.started":"2025-04-16T06:14:23.265869Z","shell.execute_reply":"2025-04-16T06:14:23.270754Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Creating Embedding DM with ChromaDB","metadata":{}},{"cell_type":"code","source":"from chromadb import Documents, EmbeddingFunction, Embeddings\nfrom google.api_core import retry\n\nfrom google.genai import types\n\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n\nclass GeminiEmbeddingFunction(EmbeddingFunction):\n    # Specify whether to generate embeddings for documents, or queries\n    document_mode = True\n\n    @retry.Retry(predicate=is_retriable)\n    def __call__(self, input: Documents) -> Embeddings:\n        if self.document_mode:\n            embedding_task = \"retrieval_document\"\n        else:\n            embedding_task = \"retrieval_query\"\n\n        response = client.models.embed_content(\n            model=\"models/text-embedding-004\",\n            contents=input,\n            config=types.EmbedContentConfig(\n                task_type=embedding_task,\n            ),\n        )\n        return [e.values for e in response.embeddings]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:14:25.892063Z","iopub.execute_input":"2025-04-16T06:14:25.892423Z","iopub.status.idle":"2025-04-16T06:14:26.677947Z","shell.execute_reply.started":"2025-04-16T06:14:25.892369Z","shell.execute_reply":"2025-04-16T06:14:26.677154Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Creating ChromaDB","metadata":{}},{"cell_type":"code","source":"import chromadb\n\nDB_NAME = \"bartenderbotdb\"\n\nembed_fn = GeminiEmbeddingFunction()\nembed_fn.document_mode = True\n\nchroma_client = chromadb.Client()\ndb = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n\ndb.add(documents=documents, ids=[str(i) for i in range(len(documents))])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:14:30.807244Z","iopub.execute_input":"2025-04-16T06:14:30.807788Z","iopub.status.idle":"2025-04-16T06:14:31.649244Z","shell.execute_reply.started":"2025-04-16T06:14:30.807751Z","shell.execute_reply":"2025-04-16T06:14:31.648522Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"confirm db","metadata":{}},{"cell_type":"code","source":"db.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:14:35.456243Z","iopub.execute_input":"2025-04-16T06:14:35.457114Z","iopub.status.idle":"2025-04-16T06:14:35.465344Z","shell.execute_reply.started":"2025-04-16T06:14:35.457084Z","shell.execute_reply":"2025-04-16T06:14:35.464461Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Retrieval","metadata":{}},{"cell_type":"code","source":"# Switch to query mode when generating embeddings.\nembed_fn.document_mode = False\n\n# Search the Chroma DB using the specified query.\nquery = \"I'm looking for something refreshing, what do you recommend?\"\n\nresult = db.query(query_texts=[query], n_results=1)\n[all_passages] = result[\"documents\"]\n\nMarkdown(all_passages[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:14:43.063223Z","iopub.execute_input":"2025-04-16T06:14:43.063587Z","iopub.status.idle":"2025-04-16T06:14:43.454884Z","shell.execute_reply.started":"2025-04-16T06:14:43.063564Z","shell.execute_reply":"2025-04-16T06:14:43.454110Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"If you're looking for something refreshing, our Long Island is always a winner."},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Augmented Generation","metadata":{}},{"cell_type":"code","source":"query_oneline = query.replace(\"\\n\", \" \")\n\n# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\nprompt = f\"\"\"You are a bartender bot at \"Mok-5-sha Bar\" that is conversational and interacts with customers\nusing text from the reference passage included below. \nBe sure to respond in a complete sentence while maintaining a modest and humorous tone. \nIf the passage is irrelevant to the answer, you may ignore it.\n\nQUESTION: {query_oneline}\n\"\"\"\n\n# Add the retrieved documents to the prompt.\nfor passage in all_passages:\n    passage_oneline = passage.replace(\"\\n\", \" \")\n    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n\nprint(prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:14:48.183854Z","iopub.execute_input":"2025-04-16T06:14:48.184186Z","iopub.status.idle":"2025-04-16T06:14:48.190618Z","shell.execute_reply.started":"2025-04-16T06:14:48.184162Z","shell.execute_reply":"2025-04-16T06:14:48.189410Z"}},"outputs":[{"name":"stdout","text":"You are a bartender bot at \"Mok-5-sha Bar\" that is concersations and interacts with customers\nusing text from the reference passage included below. \nBe sure to respond in a complete sentence while maintaining a modest and humorous tone. \nIf the passage is irrelevant to the answer, you may ignore it.\n\nQUESTION: I'm looking for something refreshing, what do you recommend?\nPASSAGE: If you're looking for something refreshing, our Long Island is always a winner.\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"answer = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=prompt)\n\nMarkdown(answer.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:16:16.547308Z","iopub.execute_input":"2025-04-16T06:16:16.548233Z","iopub.status.idle":"2025-04-16T06:16:17.285298Z","shell.execute_reply.started":"2025-04-16T06:16:16.548199Z","shell.execute_reply":"2025-04-16T06:16:17.284359Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"If you're looking for something refreshing, our Long Island is always a winner.\n"},"metadata":{}}],"execution_count":12}]}