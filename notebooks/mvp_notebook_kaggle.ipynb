{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBPtIMysHwnz"
      },
      "outputs": [],
      "source": [
        "# MOK 5-ha Bartending Agent\n",
        "# Google Colab Notebook\n",
        "\n",
        "# @title Setup and Installation\n",
        "# @markdown Run this cell to install dependencies and set up the environment\n",
        "\n",
        "# Install required packages\n",
        "!pip install google-generativeai>=0.3.0 tenacity>=8.2.3 gradio>=4.0.0 cartesia>=2.0.0 python-dotenv>=1.0.0\n",
        "\n",
        "# Import common libraries\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import re\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import gradio as gr\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from PIL import Image\n",
        "import base64\n",
        "import requests\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# @title API Keys Setup\n",
        "# @markdown Enter your API keys here\n",
        "\n",
        "gemini_api_key = \"\" # @param {type:\"string\"}\n",
        "cartesia_api_key = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# Save API keys to environment variables\n",
        "os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n",
        "os.environ[\"CARTESIA_API_KEY\"] = cartesia_api_key\n",
        "\n",
        "# @title Upload Bartender Avatar (Optional)\n",
        "# @markdown Upload a bartender avatar image or use the default\n",
        "\n",
        "use_default_avatar = True # @param {type:\"boolean\"}\n",
        "\n",
        "# Default avatar URL\n",
        "default_avatar_url = \"https://raw.githubusercontent.com/your-github-repo/MOK-5-ha/main/assets/bartender_avatar_ai_studio.jpeg\"\n",
        "\n",
        "if use_default_avatar:\n",
        "    # Download default avatar\n",
        "    try:\n",
        "        response = requests.get(default_avatar_url)\n",
        "        if response.status_code == 200:\n",
        "            avatar_bytes = response.content\n",
        "            avatar_image = Image.open(io.BytesIO(avatar_bytes))\n",
        "            print(\"Using default avatar image\")\n",
        "        else:\n",
        "            print(f\"Failed to download default avatar. Status code: {response.status_code}\")\n",
        "            # Create a blank avatar as fallback\n",
        "            avatar_image = Image.new('RGB', (300, 300), color = (73, 109, 137))\n",
        "    except Exception as e:\n",
        "        print(f\"Error using default avatar: {e}\")\n",
        "        # Create a blank avatar as fallback\n",
        "        avatar_image = Image.new('RGB', (300, 300), color = (73, 109, 137))\n",
        "else:\n",
        "    # Ask user to upload an avatar\n",
        "    print(\"Please upload an avatar image:\")\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        avatar_key = next(iter(uploaded))\n",
        "        avatar_bytes = uploaded[avatar_key]\n",
        "        avatar_image = Image.open(io.BytesIO(avatar_bytes))\n",
        "        print(f\"Uploaded avatar: {avatar_key}\")\n",
        "    else:\n",
        "        print(\"No avatar uploaded, using default\")\n",
        "        # Create a blank avatar as fallback\n",
        "        avatar_image = Image.new('RGB', (300, 300), color = (73, 109, 137))\n",
        "\n",
        "# Display the avatar\n",
        "plt.imshow(avatar_image)\n",
        "plt.axis('off')\n",
        "plt.title(\"Bartender Avatar\")\n",
        "plt.show()\n",
        "\n",
        "# Save avatar for use in Gradio\n",
        "avatar_path = \"bartender_avatar.jpg\"\n",
        "avatar_image.save(avatar_path)\n",
        "print(f\"Avatar saved to {avatar_path}\")\n",
        "\n",
        "# @title Bartending Agent Implementation\n",
        "\n",
        "# Gemini - Frontier LLM\n",
        "try:\n",
        "    # Using 'ggenai' alias consistent with user's snippets\n",
        "    import google.generativeai as ggenai\n",
        "    from google.api_core import retry as core_retry # For potential core retries\n",
        "    from google.generativeai import types as genai_types # For specific types if needed later\n",
        "except ImportError:\n",
        "    print(\"Error: google.generativeai library not found.\")\n",
        "    print(\"Please install it using: pip install google-generativeai\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Tenacity for retries on specific functions\n",
        "try:\n",
        "    from tenacity import (\n",
        "        retry as tenacity_retry, # Alias to avoid confusion with google.api_core.retry\n",
        "        stop_after_attempt,\n",
        "        wait_exponential,\n",
        "        retry_if_exception_type,\n",
        "        before_sleep_log\n",
        "    )\n",
        "except ImportError:\n",
        "    print(\"Warning: tenacity library not found. Retries on API calls will not be enabled.\")\n",
        "    print(\"Install it using: pip install tenacity\")\n",
        "    # Define a dummy decorator if tenacity is missing\n",
        "    def tenacity_retry(*args, **kwargs):\n",
        "        def decorator(func):\n",
        "            return func\n",
        "        return decorator\n",
        "    RETRYABLE_EXCEPTIONS = (Exception,) # Fallback to generic exception\n",
        "    before_sleep_log = lambda logger, level: None # Dummy function\n",
        "\n",
        "try:\n",
        "    from cartesia import Cartesia\n",
        "    from cartesia.tts import OutputFormat_Raw, TtsRequestIdSpecifier\n",
        "except ImportError:\n",
        "    print(\"Error: Cartesia library not found.\")\n",
        "    print(\"Please ensure it's installed with: pip install cartesia\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# Get API Key (Ensure this is set in your .env file or system environment)\n",
        "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "CARTESIA_API_KEY = os.getenv(\"CARTESIA_API_KEY\") # Load Cartesia key\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    logger.error(\"FATAL: GEMINI_API_KEY not found in environment variables or .env file.\")\n",
        "    raise EnvironmentError(\"GEMINI_API_KEY is required but not found.\")\n",
        "\n",
        "if not CARTESIA_API_KEY:\n",
        "    logger.error(\"FATAL: CARTESIA_API_KEY not found in environment variables or .env file.\")\n",
        "    # Decide if TTS is optional or required. Assuming required for now.\n",
        "    raise EnvironmentError(\"CARTESIA_API_KEY is required but not found.\")\n",
        "\n",
        "# Configure Gemini Client and Model (Initialized ONCE at module load)\n",
        "try:\n",
        "    ggenai.configure(api_key=GOOGLE_API_KEY)\n",
        "    # Use a valid and available model name, e.g., 'gemini-1.5-flash' or 'gemini-pro'\n",
        "    MODEL_NAME = 'gemini-2.0-flash' # Verify this model name is correct and accessible\n",
        "    model = ggenai.GenerativeModel(MODEL_NAME)\n",
        "    logger.info(f\"Successfully initialized Gemini model: {MODEL_NAME}\")\n",
        "except Exception as e:\n",
        "    logger.exception(f\"Fatal: Failed to initialize Gemini model: {str(e)}\")\n",
        "    raise RuntimeError(\n",
        "        f\"Failed to initialize Gemini model. Check API key and model name ('{MODEL_NAME}').\"\n",
        "    ) from e\n",
        "\n",
        "# Initialize Cartesia Client (ONCE at module load)\n",
        "try:\n",
        "    # Replace \"your-chosen-voice-id\" with an actual valid ID from Cartesia\n",
        "    CARTESIA_VOICE_ID = \"6f84f4b8-58a2-430c-8c79-688dad597532\" # Example placeholder ID - CHANGE THIS\n",
        "    if not CARTESIA_VOICE_ID or \"your-chosen-voice-id\" in CARTESIA_VOICE_ID:\n",
        "         logger.warning(\"CARTESIA_VOICE_ID is not set to a valid ID. Please edit bartending_agent.py.\")\n",
        "         # Decide if this is fatal. Maybe proceed without voice for now?\n",
        "\n",
        "    cartesia_client = Cartesia(\n",
        "        api_key=os.getenv(\"CARTESIA_API_KEY\"),\n",
        "        )\n",
        "    logger.info(\"Successfully initialized Cartesia client.\")\n",
        "    # Optional: Could add a check here to verify the voice ID exists using the client if possible\n",
        "except Exception as e:\n",
        "     logger.exception(\"Fatal: Failed to initialize Cartesia client.\")\n",
        "     raise RuntimeError(\"Cartesia client initialization failed.\") from e\n",
        "\n",
        "# --- Static Data ---\n",
        "# Define the Menu (Doesn't change per session)\n",
        "menu: Dict[str, Dict[str, float]] = {\n",
        "    \"1\": {\"name\": \"Old Fashioned\", \"price\": 12.00},\n",
        "    \"2\": {\"name\": \"Margarita\", \"price\": 10.00},\n",
        "    \"3\": {\"name\": \"Mojito\", \"price\": 11.00},\n",
        "    \"4\": {\"name\": \"Martini\", \"price\": 13.00},\n",
        "    \"5\": {\"name\": \"Whiskey Sour\", \"price\": 11.00},\n",
        "    \"6\": {\"name\": \"Gin and Tonic\", \"price\": 9.00},\n",
        "    \"7\": {\"name\": \"Manhattan\", \"price\": 12.00},\n",
        "    \"8\": {\"name\": \"Daiquiri\", \"price\": 10.00},\n",
        "    \"9\": {\"name\": \"Negroni\", \"price\": 11.00},\n",
        "    \"10\": {\"name\": \"Cosmopolitan\", \"price\": 12.00}\n",
        "}\n",
        "\n",
        "def get_menu_text() -> str:\n",
        "    \"\"\"Generates the menu text (Stateless).\"\"\"\n",
        "    global menu # Access the global menu variable\n",
        "    menu_text = \"Menu:\\n\" + \"-\"*5 + \"\\n\"\n",
        "    for item_id, item in menu.items():\n",
        "        menu_text += f\"{item_id}. {item['name']} - ${item['price']:.2f}\\n\"\n",
        "    return menu_text\n",
        "\n",
        "@tenacity_retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
        "    #retry=retry_if_exception_type(RETRYABLE_EXCEPTIONS),\n",
        "    before_sleep=before_sleep_log(logger, logging.WARNING) if callable(before_sleep_log) else None, # Check if callable\n",
        "    reraise=True # Re-raise the exception if all retries fail\n",
        ")\n",
        "def _call_gemini_api(prompt_content: List[str], config: Dict) -> ggenai.types.GenerateContentResponse:\n",
        "    \"\"\"Internal function to call the Gemini API with retry logic (Stateless).\"\"\"\n",
        "    logger.debug(\"Calling Gemini API...\")\n",
        "    # Uses the globally initialized 'model'\n",
        "    response = model.generate_content(\n",
        "        contents=prompt_content, # Correct parameter name is 'contents'\n",
        "        generation_config=config,\n",
        "        # safety_settings can be added here if needed\n",
        "    )\n",
        "    logger.debug(\"Gemini API call successful.\")\n",
        "    return response\n",
        "\n",
        "def process_order(\n",
        "    user_input_text: str,\n",
        "    current_session_history: List[Dict[str, str]],\n",
        "    current_session_order: List[Dict[str, float]]\n",
        ") -> Tuple[str, List[Dict[str, str]], List[Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Processes user input using Gemini, updates state for the CURRENT SESSION.\n",
        "    Accepts session history and order, returns (response_text, updated_history, updated_order).\n",
        "    \"\"\"\n",
        "    global menu # Allow access to the global menu\n",
        "\n",
        "    if not user_input_text:\n",
        "        logger.warning(\"Received empty user input.\")\n",
        "        # Return current state unchanged with a message\n",
        "        return \"Please tell me what you'd like to order.\", current_session_history, current_session_order\n",
        "\n",
        "    # Local copies for modification within this function call - ensures statelessness\n",
        "    # Regarding the input arguments. The returned values become the new state.\n",
        "    updated_history = current_session_history[:]\n",
        "    updated_order = current_session_order[:]\n",
        "\n",
        "    try:\n",
        "        # --- Construct the prompt using session-specific history/order ---\n",
        "        prompt_context = [\n",
        "            \"You are a friendly and helpful bartender taking drink orders.\",\n",
        "            \"Be conversational. Ask clarifying questions if the order is unclear.\",\n",
        "            \"If the user asks for something not on the menu, politely tell them and show the menu again.\",\n",
        "            \"If the user asks to see their current order, list the items and their prices.\",\n",
        "            \"The bar's name is MOK 5-ha, pronounced as 'Moksha'. If a customer asks about the name, explain that:\",\n",
        "            \"Moksha represents liberation from the cycle of rebirth (samsara) and union with the divine. It is achieved through spiritual enlightenment, freeing the soul from karma and earthly attachments to attain eternal bliss.\",\n",
        "            \"\\nHere is the menu:\",\n",
        "            get_menu_text(), # Call the stateless menu function\n",
        "            \"\\nCurrent order:\",\n",
        "        ]\n",
        "        if updated_order: # Use the passed-in order state copy\n",
        "            order_text = \"\\n\".join([f\"- {item['name']} (${item['price']:.2f})\" for item in updated_order])\n",
        "            prompt_context.append(order_text)\n",
        "        else:\n",
        "            prompt_context.append(\"No items ordered yet.\")\n",
        "\n",
        "        prompt_context.append(\"\\nConversation History (latest turns):\")\n",
        "        history_limit = 10 # Keep the last ~5 pairs of interactions\n",
        "        limited_history_for_prompt = updated_history[-history_limit:] # Use passed-in history state copy\n",
        "\n",
        "        for entry in limited_history_for_prompt:\n",
        "             role = entry.get(\"role\", \"unknown\").capitalize()\n",
        "             content = entry.get(\"content\", \"\")\n",
        "             prompt_context.append(f\"{role}: {content}\")\n",
        "\n",
        "        # Add the current user input to the prompt context\n",
        "        prompt_context.append(f\"\\nUser: {user_input_text}\")\n",
        "        prompt_context.append(\"\\nBartender:\") # Ask the model to reply as the bartender\n",
        "\n",
        "        full_prompt = \"\\n\".join(prompt_context)\n",
        "        logger.info(f\"Processing user input for session: {user_input_text}\")\n",
        "        logger.debug(f\"Full prompt for Gemini:\\n------\\n{full_prompt}\\n------\")\n",
        "\n",
        "        # --- Call Gemini API via the retry wrapper ---\n",
        "        config_dict = {\n",
        "            'temperature': 0.7,\n",
        "            'max_output_tokens': 2048,\n",
        "            # 'candidate_count': 1 # Usually defaults to 1\n",
        "        }\n",
        "        response = _call_gemini_api(prompt_content=[full_prompt], config=config_dict)\n",
        "\n",
        "        # --- Process the response ---\n",
        "        agent_response_text = \"\" # Default empty response\n",
        "\n",
        "        # Check response validity and safety\n",
        "        if not response.candidates:\n",
        "             logger.error(\"Gemini response has no candidates.\")\n",
        "             if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                 logger.error(f\"Prompt Blocked: {response.prompt_feedback.block_reason_message}\")\n",
        "                 agent_response_text = f\"I'm sorry, my ability to respond was blocked. Reason: {response.prompt_feedback.block_reason_message or response.prompt_feedback.block_reason}\"\n",
        "             else:\n",
        "                 agent_response_text = \"Sorry, I couldn't generate a response. Please try again.\"\n",
        "\n",
        "        elif not response.candidates[0].content or not response.candidates[0].content.parts:\n",
        "             logger.error(\"Gemini response candidate is empty or has no parts.\")\n",
        "             finish_reason = response.candidates[0].finish_reason\n",
        "             finish_reason_name = finish_reason.name if finish_reason else 'UNKNOWN'\n",
        "             logger.error(f\"Finish Reason: {finish_reason_name}\")\n",
        "\n",
        "             if finish_reason_name == \"SAFETY\":\n",
        "                 agent_response_text = \"I'm sorry, I can't provide that response due to safety reasons.\"\n",
        "             elif finish_reason_name == \"RECITATION\":\n",
        "                 agent_response_text = \"My response couldn't be completed due to potential recitation issues.\"\n",
        "             elif finish_reason_name == \"MAX_TOKENS\":\n",
        "                 try: # Attempt to get partial text if stopped due to length\n",
        "                     agent_response_text = response.candidates[0].content.parts[0].text + \"... (response truncated)\"\n",
        "                     logger.warning(\"Response truncated due to max_tokens.\")\n",
        "                 except (AttributeError, IndexError):\n",
        "                     agent_response_text = \"My response was cut short as it reached the maximum length.\"\n",
        "             else:\n",
        "                agent_response_text = f\"Sorry, I had trouble generating a complete response (Finish Reason: {finish_reason_name}). Could you rephrase?\"\n",
        "        else:\n",
        "             # Successfully got response text\n",
        "             agent_response_text = response.candidates[0].content.parts[0].text\n",
        "             logger.info(f\"Gemini response received: {agent_response_text}\")\n",
        "\n",
        "             # --- Update Order Based on Response (Heuristic) ---\n",
        "             # Modifies the 'updated_order' local variable\n",
        "             for item_id, item in menu.items():\n",
        "                 item_name_lower = item[\"name\"].lower()\n",
        "                 response_lower = agent_response_text.lower()\n",
        "                 if item_name_lower in response_lower and \\\n",
        "                    any(add_word in response_lower for add_word in [\"added\", \"adding\", \"got it\", \"sure thing\", \"order up\", \"coming right up\"]):\n",
        "                      # Avoid adding duplicates if it's already the *last* item added\n",
        "                      if not updated_order or item[\"name\"] != updated_order[-1][\"name\"]:\n",
        "                          updated_order.append(item) # Append to local copy\n",
        "                          logger.info(f\"Heuristic: Added '{item['name']}' to session order.\")\n",
        "                          break # Only add the first match found\n",
        "\n",
        "        # --- Update Session History ---\n",
        "        # Append user input and assistant response to the local history copy\n",
        "        # This prepares the history to be returned as the new state\n",
        "        updated_history.append({'role': 'user', 'content': user_input_text})\n",
        "        updated_history.append({'role': 'assistant', 'content': agent_response_text})\n",
        "\n",
        "        # --- Return updated state for this session ---\n",
        "        return agent_response_text, updated_history, updated_order\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch exceptions not handled by tenacity retry\n",
        "        logger.exception(f\"Critical error in process_order: {str(e)}\")\n",
        "        # Provide a safe fallback response and state\n",
        "        error_message = \"I'm sorry, an unexpected error occurred. Please try again later.\"\n",
        "        # Append only the error message to history to inform the user\n",
        "        # Make sure not to corrupt the state further\n",
        "        safe_history = current_session_history[:] # Revert to original history for this turn\n",
        "        safe_history.append({'role': 'user', 'content': user_input_text}) # Keep user msg\n",
        "        safe_history.append({'role': 'assistant', 'content': error_message})\n",
        "        return error_message, safe_history, current_session_order # Return original order state on error\n",
        "\n",
        "# Define retryable exceptions for Cartesia if known, otherwise use generic ones\n",
        "# Example: CARTESIA_RETRYABLE_EXCEPTIONS = (cartesia.errors.ServerError, cartesia.errors.RateLimitError, ConnectionError)\n",
        "# Using generic exceptions for now as specific Cartesia ones aren't known here.\n",
        "CARTESIA_RETRYABLE_EXCEPTIONS = (ConnectionError, TimeoutError) # Add more specific Cartesia errors if documented\n",
        "\n",
        "@tenacity_retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=1, min=1, max=5),\n",
        "    retry=retry_if_exception_type(CARTESIA_RETRYABLE_EXCEPTIONS),\n",
        "    before_sleep=before_sleep_log(logger, logging.WARNING) if callable(before_sleep_log) else None,\n",
        "    reraise=True\n",
        ")\n",
        "def get_voice_audio(text_to_speak: str) -> bytes | None:\n",
        "    \"\"\"Calls Cartesia API synchronously to synthesize speech and returns WAV bytes.\"\"\"\n",
        "    global cartesia_client, CARTESIA_VOICE_ID # Access the global client and voice ID\n",
        "\n",
        "    if not text_to_speak or not text_to_speak.strip():\n",
        "        logger.warning(\"get_voice_audio received empty text.\")\n",
        "        return None\n",
        "    if not cartesia_client or not CARTESIA_VOICE_ID:\n",
        "         logger.error(\"Cartesia client or voice ID not initialized, cannot generate audio.\")\n",
        "         return None\n",
        "\n",
        "    try:\n",
        "        # Replace \"MOK 5-ha\" with \"Moksha\" for pronunciation in TTS\n",
        "        text_for_tts = re.sub(r'MOK 5-ha', 'Moksha', text_to_speak, flags=re.IGNORECASE)\n",
        "        if text_for_tts != text_to_speak:\n",
        "            logger.info(\"Applied 'MOK 5-ha' â†’ 'Moksha' pronunciation for TTS.\")\n",
        "            \n",
        "        logger.info(f\"Requesting TTS from Cartesia (Voice ID: {CARTESIA_VOICE_ID}) for: '{text_for_tts[:50]}...'\")\n",
        "\n",
        "        # --- Check Cartesia Documentation for the exact method call ---\n",
        "        # This is a plausible synchronous implementation pattern:\n",
        "        audio_generator = cartesia_client.tts.bytes(\n",
        "            model_id=\"sonic-2\",\n",
        "            transcript=text_for_tts,  # Use the modified text with correct pronunciation\n",
        "            voice={\"mode\":\"id\",\n",
        "                   \"id\": CARTESIA_VOICE_ID,\n",
        "            },\n",
        "            language=\"en\",\n",
        "            # Specify desired output format and sample rate\n",
        "            output_format={\"container\":\"wav\",\n",
        "                           \"sample_rate\": 24000,\n",
        "                           \"encoding\": \"pcm_f32le\",\n",
        "            },\n",
        "        )\n",
        "\n",
        "        # Concatenate chunks from the generator for a blocking result\n",
        "        audio_data = b\"\".join(chunk for chunk in audio_generator)\n",
        "        # --- End of section requiring Cartesia documentation check ---\n",
        "\n",
        "        if not audio_data:\n",
        "            logger.warning(\"Cartesia TTS returned empty audio data.\")\n",
        "            return None\n",
        "\n",
        "        logger.info(f\"Received {len(audio_data)} bytes of WAV audio data from Cartesia.\")\n",
        "        return audio_data\n",
        "\n",
        "    # Catch specific Cartesia errors if they exist and are imported\n",
        "    # except cartesia.errors.CartesiaError as e:\n",
        "    #    logger.exception(f\"Cartesia API error during TTS generation: {e}\")\n",
        "    #    return None\n",
        "    except Exception as e:\n",
        "        # Catch any other unexpected error during TTS\n",
        "        logger.exception(f\"Unexpected error generating voice audio with Cartesia: {e}\")\n",
        "        return None\n",
        "\n",
        "# @title Gradio Interface Implementation\n",
        "\n",
        "def handle_gradio_input(\n",
        "    user_input: str,\n",
        "    session_history_state: List[Dict[str, str]],\n",
        "    session_order_state: List[Dict[str, float]]\n",
        ") -> Tuple[str, List[Dict[str, str]], List[Dict[str, str]], List[Dict[str, float]], Any]:\n",
        "    \"\"\"\n",
        "    Gradio callback: Takes input/state, calls logic & TTS, returns updates.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Gradio input: '{user_input}'\")\n",
        "    logger.debug(f\"Received session history state (len {len(session_history_state)}): {session_history_state}\")\n",
        "    logger.debug(f\"Received session order state (len {len(session_order_state)}): {session_order_state}\")\n",
        "\n",
        "    # Call text processing logic first\n",
        "    response_text, updated_history, updated_order = process_order(\n",
        "        user_input,\n",
        "        session_history_state,\n",
        "        session_order_state\n",
        "    )\n",
        "\n",
        "    # --- Get Voice Audio ---\n",
        "    audio_data = None # Default to None\n",
        "    # Check if there is a non-empty response text to synthesize\n",
        "    if response_text and response_text.strip():\n",
        "         audio_data = get_voice_audio(response_text) # Call the imported function\n",
        "         if audio_data is None:\n",
        "             logger.warning(\"Failed to get audio data from get_voice_audio.\")\n",
        "             # Optional: Add indication to user? E.g., append \"[Audio failed]\" to response_text\n",
        "    else:\n",
        "        logger.info(\"No response text generated, skipping TTS.\")\n",
        "    # --- End Get Voice Audio ---\n",
        "\n",
        "    # Return updates including audio data (which might be None)\n",
        "    return \"\", updated_history, updated_history, updated_order, audio_data\n",
        "\n",
        "def clear_chat_state() -> Tuple[List, List, List, None]:\n",
        "    \"\"\"Clears UI/session state including audio.\"\"\"\n",
        "    logger.info(\"Clear button clicked - Resetting session state.\")\n",
        "    # Return empty lists for Chatbot/history/order, and None for the audio component\n",
        "    return [], [], [], None\n",
        "\n",
        "# @title Launch the Gradio Interface\n",
        "\n",
        "def launch_bartender_interface():\n",
        "    theme = gr.themes.Citrus()\n",
        "\n",
        "    with gr.Blocks(theme=theme) as demo:\n",
        "        gr.Markdown(\"# MOK 5-ha Bartending Agent\")\n",
        "        gr.Markdown(\"Welcome to MOK 5-ha! Ask me for a drink or check your order.\")\n",
        "\n",
        "        # --- Define Session State Variables ---\n",
        "        history_state = gr.State([])\n",
        "        order_state = gr.State([])\n",
        "\n",
        "        # --- Restructured Main Row with 2 Columns (Equal Scaling) ---\n",
        "        with gr.Row():\n",
        "\n",
        "            # --- Column 1: Avatar Image ---\n",
        "            # Scale is relative to other columns in the same row\n",
        "            with gr.Column(scale=1, min_width=200): # Keep scale=1\n",
        "                gr.Image(\n",
        "                    value=avatar_path,  # Use the saved avatar path\n",
        "                    label=\"Bartender Avatar\",\n",
        "                    show_label=False,\n",
        "                    interactive=False,\n",
        "                    height=600, # Adjust as desired\n",
        "                    elem_classes=[\"avatar-image\"]\n",
        "                )\n",
        "\n",
        "            # --- Column 2: Chat Interface ---\n",
        "            with gr.Column(scale=1): # <-- Changed scale from 3 to 1\n",
        "                chatbot_display = gr.Chatbot(\n",
        "                    [],\n",
        "                    elem_id=\"chatbot\",\n",
        "                    label=\"Conversation\",\n",
        "                    bubble_full_width=False,\n",
        "                    height=450, # Keep or adjust height for rectangular shape\n",
        "                    type=\"messages\"\n",
        "                )\n",
        "                agent_audio_output = gr.Audio(\n",
        "                    label=\"Agent Voice\",\n",
        "                    autoplay=True,\n",
        "                    streaming=False,\n",
        "                    format=\"wav\",\n",
        "                    show_label=True,\n",
        "                    interactive=False\n",
        "                )\n",
        "                msg_input = gr.Textbox(\n",
        "                    label=\"Your Order / Message\",\n",
        "                    placeholder=\"What can I get for you? (e.g., 'I'd like a Margarita', 'Show my order')\"\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    clear_btn = gr.Button(\"Clear Conversation\")\n",
        "                    submit_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "\n",
        "        # --- Event Handlers (Remain the same) ---\n",
        "        submit_inputs = [msg_input, history_state, order_state]\n",
        "        submit_outputs = [msg_input, chatbot_display, history_state, order_state, agent_audio_output]\n",
        "        msg_input.submit(handle_gradio_input, submit_inputs, submit_outputs)\n",
        "        submit_btn.click(handle_gradio_input, submit_inputs, submit_outputs)\n",
        "\n",
        "        clear_outputs = [chatbot_display, history_state, order_state, agent_audio_output]\n",
        "        clear_btn.click(clear_chat_state, None, clear_outputs)\n",
        "\n",
        "    # Launch the interface\n",
        "    demo.launch(debug=True, share=True)  # share=True to make it accessible via a public URL\n",
        "\n",
        "# @title Run the Bartending Agent\n",
        "# @markdown Click to run the bartending agent interface\n",
        "\n",
        "# Launch the interface when this cell is executed\n",
        "launch_bartender_interface()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
